{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "639206003e2f0bda6dba87f24f406417",
     "grade": false,
     "grade_id": "cell-23c6dcc8cfed1b81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import platform\n",
    "from six.moves import cPickle as pickle\n",
    "from builtins import range\n",
    "\n",
    "def rel_error(x, y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == \"2\":\n",
    "        return pickle.load(f)\n",
    "    elif version[0] == \"3\":\n",
    "        return pickle.load(f, encoding=\"latin1\")\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict[\"data\"]\n",
    "        Y = datadict[\"labels\"]\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(ROOT, \"data_batch_%d\" % (b,))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, \"test_batch\"))\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    fx = f(x)  # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h  # increment by h\n",
    "        fxph = f(x)  # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x)  # evaluate f(x - h)\n",
    "        x[ix] = oldval  # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h)  # the slope\n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext()  # step to next dimension\n",
    "\n",
    "    return grad\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, subtract_mean=True):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = os.path.join(\"/root/cifar-10-batches-py\")\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    \n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    if subtract_mean:\n",
    "        mean_image = np.mean(X_train, axis=0)\n",
    "        X_train -= mean_image\n",
    "        X_val -= mean_image\n",
    "        X_test -= mean_image\n",
    "\n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
    "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
    "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
    "\n",
    "    # Package data into a dictionary\n",
    "    return {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "    }\n",
    "\n",
    "def sgd(w, dw, config=None):\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    config.setdefault(\"learning_rate\", 1e-2)\n",
    "    w -= config[\"learning_rate\"]*dw\n",
    "    return w, config\n",
    "\n",
    "class Solver(object):\n",
    "    def __init__(self, model, data, **kwargs):\n",
    "        \"\"\"\n",
    "        Construct a new Solver instance.\n",
    "\n",
    "        Required arguments:\n",
    "        - model: A model object conforming to the API described above\n",
    "        - data: A dictionary of training and validation data containing:\n",
    "          'X_train': Array, shape (N_train, d_1, ..., d_n) of training images\n",
    "          'X_val': Array, shape (N_val, d_1, ..., d_n) of validation images\n",
    "          'y_train': Array, shape (N_train,) of labels for training images\n",
    "          'y_val': Array, shape (N_val,) of labels for validation images\n",
    "\n",
    "        Optional arguments:\n",
    "        - optim_config: A dictionary containing hyperparameters that will be\n",
    "          passed to the chosen update rule. Each update rule requires different\n",
    "          hyperparameters  but all update rules require a\n",
    "          'learning_rate' parameter so that should always be present.\n",
    "        - lr_decay: A scalar for learning rate decay; after each epoch the\n",
    "          learning rate is multiplied by this value.\n",
    "        - batch_size: Size of minibatches used to compute loss and gradient\n",
    "          during training.\n",
    "        - num_epochs: The number of epochs to run for during training.\n",
    "        - print_every: Integer; training losses will be printed every\n",
    "          print_every iterations.\n",
    "        - verbose: Boolean; if set to false then no output will be printed\n",
    "          during training.\n",
    "        - num_train_samples: Number of training samples used to check training\n",
    "          accuracy; default is 1000; set to None to use entire training set.\n",
    "        - num_val_samples: Number of validation samples to use to check val\n",
    "          accuracy; default is None, which uses the entire validation set.\n",
    "        - checkpoint_name: If not None, then save model checkpoints here every\n",
    "          epoch.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.X_train = data[\"X_train\"]\n",
    "        self.y_train = data[\"y_train\"]\n",
    "        self.X_val = data[\"X_val\"]\n",
    "        self.y_val = data[\"y_val\"]\n",
    "\n",
    "        # Unpack keyword arguments\n",
    "        self.optim_config = kwargs.pop(\"optim_config\", {})\n",
    "        self.lr_decay = kwargs.pop(\"lr_decay\", 1.0)\n",
    "        self.batch_size = kwargs.pop(\"batch_size\", 100)\n",
    "        self.num_epochs = kwargs.pop(\"num_epochs\", 10)\n",
    "        self.num_train_samples = kwargs.pop(\"num_train_samples\", 1000)\n",
    "        self.num_val_samples = kwargs.pop(\"num_val_samples\", None)\n",
    "\n",
    "        self.checkpoint_name = kwargs.pop(\"checkpoint_name\", None)\n",
    "        self.print_every = kwargs.pop(\"print_every\", 10)\n",
    "        self.verbose = kwargs.pop(\"verbose\", True)\n",
    "\n",
    "        # Throw an error if there are extra keyword arguments\n",
    "        if len(kwargs) > 0:\n",
    "            extra = \", \".join('\"%s\"' % k for k in list(kwargs.keys()))\n",
    "            raise ValueError(\"Unrecognized arguments %s\" % extra)\n",
    "\n",
    "        self.update_rule = sgd\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        # Set up some variables for book-keeping\n",
    "        self.epoch = 0\n",
    "        self.best_val_acc = 0\n",
    "        self.best_params = {}\n",
    "        self.loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.val_acc_history = []\n",
    "\n",
    "        # Make a deep copy of the optim_config for each parameter\n",
    "        self.optim_configs = {}\n",
    "        for p in self.model.params:\n",
    "            d = {k: v for k, v in self.optim_config.items()}\n",
    "            self.optim_configs[p] = d\n",
    "\n",
    "    def _step(self):\n",
    "        # Make a minibatch of training data\n",
    "        num_train = self.X_train.shape[0]\n",
    "        batch_mask = np.random.choice(num_train, self.batch_size)\n",
    "        X_batch = self.X_train[batch_mask]\n",
    "        y_batch = self.y_train[batch_mask]\n",
    "\n",
    "        # Compute loss and gradient\n",
    "        loss, grads = self.model.loss(X_batch, y_batch)\n",
    "        self.loss_history.append(loss)\n",
    "\n",
    "        # Perform a parameter update\n",
    "        for p, w in self.model.params.items():\n",
    "            dw = grads[p]\n",
    "            config = self.optim_configs[p]\n",
    "            next_w, next_config = self.update_rule(w, dw, config)\n",
    "            self.model.params[p] = next_w\n",
    "            self.optim_configs[p] = next_config\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        if self.checkpoint_name is None:\n",
    "            return\n",
    "        checkpoint = {\n",
    "            \"model\": self.model,\n",
    "            \"update_rule\": self.update_rule,\n",
    "            \"lr_decay\": self.lr_decay,\n",
    "            \"optim_config\": self.optim_config,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"num_train_samples\": self.num_train_samples,\n",
    "            \"num_val_samples\": self.num_val_samples,\n",
    "            \"epoch\": self.epoch,\n",
    "            \"loss_history\": self.loss_history,\n",
    "            \"train_acc_history\": self.train_acc_history,\n",
    "            \"val_acc_history\": self.val_acc_history,\n",
    "        }\n",
    "        filename = \"%s_epoch_%d.pkl\" % (self.checkpoint_name, self.epoch)\n",
    "        if self.verbose:\n",
    "            print('Saving checkpoint to \"%s\"' % filename)\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(checkpoint, f)\n",
    "\n",
    "    def check_accuracy(self, X, y, num_samples=None, batch_size=100):\n",
    "        # Maybe subsample the data\n",
    "        N = X.shape[0]\n",
    "        if num_samples is not None and N > num_samples:\n",
    "            mask = np.random.choice(N, num_samples)\n",
    "            N = num_samples\n",
    "            X = X[mask]\n",
    "            y = y[mask]\n",
    "\n",
    "        # Compute predictions in batches\n",
    "        num_batches = N // batch_size\n",
    "        if N % batch_size != 0:\n",
    "            num_batches += 1\n",
    "        y_pred = []\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "            scores = self.model.loss(X[start:end])\n",
    "            y_pred.append(np.argmax(scores, axis=1))\n",
    "        y_pred = np.hstack(y_pred)\n",
    "        acc = np.mean(y_pred == y)\n",
    "\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        num_train = self.X_train.shape[0]\n",
    "        iterations_per_epoch = max(num_train // self.batch_size, 1)\n",
    "        num_iterations = self.num_epochs * iterations_per_epoch\n",
    "\n",
    "        for t in range(num_iterations):\n",
    "            self._step()\n",
    "\n",
    "            # Maybe print training loss\n",
    "            if self.verbose and t % self.print_every == 0:\n",
    "                print(\n",
    "                    \"(Iteration %d / %d) loss: %f\"\n",
    "                    % (t + 1, num_iterations, self.loss_history[-1])\n",
    "                )\n",
    "\n",
    "            # At the end of every epoch, increment the epoch counter and decay\n",
    "            # the learning rate.\n",
    "            epoch_end = (t + 1) % iterations_per_epoch == 0\n",
    "            if epoch_end:\n",
    "                self.epoch += 1\n",
    "                for k in self.optim_configs:\n",
    "                    self.optim_configs[k][\"learning_rate\"] *= self.lr_decay\n",
    "\n",
    "            # Check train and val accuracy on the first iteration, the last\n",
    "            # iteration, and at the end of each epoch.\n",
    "            first_it = t == 0\n",
    "            last_it = t == num_iterations - 1\n",
    "            if first_it or last_it or epoch_end:\n",
    "                train_acc = self.check_accuracy(\n",
    "                    self.X_train, self.y_train, num_samples=self.num_train_samples\n",
    "                )\n",
    "                val_acc = self.check_accuracy(\n",
    "                    self.X_val, self.y_val, num_samples=self.num_val_samples\n",
    "                )\n",
    "                self.train_acc_history.append(train_acc)\n",
    "                self.val_acc_history.append(val_acc)\n",
    "                self._save_checkpoint()\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(\n",
    "                        \"(Epoch %d / %d) train acc: %f; val_acc: %f\"\n",
    "                        % (self.epoch, self.num_epochs, train_acc, val_acc)\n",
    "                    )\n",
    "\n",
    "                # Keep track of the best model\n",
    "                if val_acc > self.best_val_acc:\n",
    "                    self.best_val_acc = val_acc\n",
    "                    self.best_params = {}\n",
    "                    for k, v in self.model.params.items():\n",
    "                        self.best_params[k] = v.copy()\n",
    "\n",
    "        # At the end of training swap the best params into the model\n",
    "        self.model.params = self.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f923a33b246644a40e179503df62ad8b",
     "grade": false,
     "grade_id": "cell-f4c2970d97d0e3f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Forward pass for linear layer \n",
    "    N is number of examples (batch size)\n",
    "    D = d_1 * ... * d_n\n",
    "    M is output dimension\n",
    "    \n",
    "    Inputs:\n",
    "    - X: array containing input data of shape (N, d_1, ..., d_n)\n",
    "    - W: array of weights of shape (D, M)\n",
    "    - b: array of biases of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (X, W, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    # Reshape into vector before applying linear transformation\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    X_reshape = np.reshape(X, (X.shape[0], -1))\n",
    "    out = X_reshape@W + b\n",
    "    cache = (X, W, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83c05137a381c9f63fdbb130c754e5ba",
     "grade": true,
     "grade_id": "cell-dd3c33f8048f6df4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.7698500479884e-10\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "out, _ = linear_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "error = rel_error(out, correct_out)\n",
    "print(error)\n",
    "assert error < 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bea4a47f88a4f8168224afe44de785e1",
     "grade": false,
     "grade_id": "cell-8cc209b9364885bf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for linear layer layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream gradient of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - X: array of shape (N, d_1, ... d_n)\n",
    "      - W: array of weights of shape (D, M)\n",
    "      - b: array biases of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dX: Gradient with respect to x of shape (N, d1, ..., d_k)\n",
    "    - dW: Gradient with respect to w of shape (D, M)\n",
    "    - db: Gradient with respect to b of shape (M,)\n",
    "    \"\"\"\n",
    "    X, W, b = cache\n",
    "    X_reshape = np.reshape(X, (X.shape[0], -1))\n",
    "    dX, dW, db = None, None, None\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    dW = X_reshape.T@dout\n",
    "    db = np.sum(dout, axis=0)\n",
    "    dX = dout@W.T\n",
    "    dX = np.reshape(dX, X.shape)\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2979deeb80472cdd6ed6755df90354df",
     "grade": true,
     "grade_id": "cell-022800e198291a1c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dX error:  2.4408840841167563e-10\n",
      "dW error:  6.193103439209376e-11\n",
      "db error:  1.4539016235043804e-11\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(47)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "dx_num = eval_numerical_gradient_array(lambda x: linear_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: linear_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: linear_forward(x, w, b)[0], b, dout)\n",
    "_, cache = linear_forward(x, w, b)\n",
    "dx, dw, db = linear_backward(dout, cache)\n",
    "error_dX = rel_error(dx_num, dx)\n",
    "error_dW = rel_error(dw_num, dw)\n",
    "error_db = rel_error(db_num, db)\n",
    "print('dX error: ', error_dX)\n",
    "assert error_dX < 1e-9\n",
    "print('dW error: ', error_dW)\n",
    "assert error_dW < 1e-9\n",
    "print('db error: ', error_db)\n",
    "assert error_db < 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "836006f0fd28921413c2d9622faf52e7",
     "grade": false,
     "grade_id": "cell-0f1ee10dbfb7fe7e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu_forward(X):\n",
    "    \"\"\"\n",
    "    Forward pass for rectified linear units (ReLUs)layer\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs \n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output of the same shape as X\n",
    "    - cache: X\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    out = np.maximum(X, 0)\n",
    "    cache = X\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5217adeffab6864b995866047633fb3",
     "grade": true,
     "grade_id": "cell-2bba3b9f56bf1d31",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.999999798022158e-08\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "error = rel_error(out, correct_out)\n",
    "print(error)\n",
    "assert error < 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a51512c0cdc008873145d9441c19286",
     "grade": false,
     "grade_id": "cell-cda46625d06592ba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for rectified linear units (ReLUs) layer\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream gradient\n",
    "    - cache: Input X of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dX: Gradient with respect to X\n",
    "    \"\"\"\n",
    "    dX, X = None, cache\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    dX = np.sign(np.maximum(0, X))*dout\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07566a8ebce1350c78d4606254df019d",
     "grade": true,
     "grade_id": "cell-d444dc071d8dff1c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2756399622829256e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(47)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "error = rel_error(dx_num, dx)\n",
    "print(error)\n",
    "assert error < 1e-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6b699620598d8eee6b03c33ed3282e4",
     "grade": false,
     "grade_id": "cell-eeb09a83a2d66fb4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_relu_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Linear transform followed by a ReLU\n",
    "\n",
    "    Inputs:\n",
    "    - X: Input to the affine layer\n",
    "    - W, b: Weights for the affine layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    out, linear_cache, relu_cache = None, None, None\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    out_forward, linear_cache = linear_forward(x, w, b)\n",
    "    out, relu_cache = relu_forward(out_forward)\n",
    "    \n",
    "    cache = (linear_cache, relu_cache)\n",
    "    return out, cache\n",
    "\n",
    "def linear_relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the linear-relu layer\n",
    "    \"\"\"\n",
    "    fc_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = linear_backward(da, fc_cache)\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09f3663e988122bd96f7b94086b25de3",
     "grade": true,
     "grade_id": "cell-88464430f147875d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dX error:  7.014478385971623e-09\n",
      "dW error:  1.841788863087595e-09\n",
      "db error:  1.8928925550934774e-11\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(47)\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "out, cache = linear_relu_forward(x, w, b)\n",
    "dx, dw, db = linear_relu_backward(dout, cache)\n",
    "dx_num = eval_numerical_gradient_array(lambda x: linear_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: linear_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: linear_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "error_dX = rel_error(dx_num, dx)\n",
    "error_dW = rel_error(dw_num, dw)\n",
    "error_db = rel_error(db_num, db)\n",
    "print('dX error: ', error_dX)\n",
    "assert error_dX < 1e-8\n",
    "print('dW error: ', error_dW)\n",
    "assert error_dW < 1e-8\n",
    "print('db error: ', error_db)\n",
    "assert error_db < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f6f7bc389dd4aadc1116da4b4a1df9c",
     "grade": false,
     "grade_id": "cell-eec49cf37c779937",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def svm_loss(X, y):\n",
    "    loss, dX = None, None\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    N = x.shape[0]\n",
    "    correct_class_scores = x[np.arange(N), y]\n",
    "    margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0)\n",
    "    margins[np.arange(N), y] = 0\n",
    "    loss = np.sum(margins) / N\n",
    "    num_pos = np.sum(margins > 0, axis=1)\n",
    "    dX = np.zeros_like(x)\n",
    "    dX[margins > 0] = 1\n",
    "    dX[np.arange(N), y] -= num_pos\n",
    "    dX /= N\n",
    "    return loss, dX\n",
    "    \n",
    "\n",
    "def softmax_loss(X, y):\n",
    "    shifted_logits = x - np.max(x, axis=1, keepdims=True)\n",
    "    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
    "    log_probs = shifted_logits - np.log(Z)\n",
    "    probs = np.exp(log_probs)\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(log_probs[np.arange(N), y]) / N\n",
    "    dx = probs.copy()\n",
    "    dx[np.arange(N), y] -= 1\n",
    "    dx /= N\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM dX error:  1.4021566006651672e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "svm_dX_error = rel_error(dx_num, dx)\n",
    "print('SVM dX error: ', svm_dX_error)\n",
    "assert svm_dX_error < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf216bb476cb9e764e6cc1776f539aa5",
     "grade": true,
     "grade_id": "cell-1f0052067c774e4a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM dX error:  1.4021566006651672e-09\n",
      "dx error:  9.384673161989355e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "svm_dX_error = rel_error(dx_num, dx)\n",
    "print('SVM dX error: ', svm_dX_error)\n",
    "assert svm_dX_error < 1e-8\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "softmax_dX_error = rel_error(dx_num, dx)\n",
    "print('dx error: ', softmax_dX_error)\n",
    "assert softmax_dX_error < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcfbdf2c9a090cbd6674d689db7814ce",
     "grade": false,
     "grade_id": "cell-0a331e618da6a584",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Two-layer neural network with ReLU nonlinearity and softmax loss \n",
    "    D = number of features\n",
    "    H = hidden dimenion\n",
    "    C = number of classes\n",
    "\n",
    "    Architecure is affine - relu - affine - softmax.\n",
    "\n",
    "    Parameters are stored in the dictionary self.params mapping parameter names to numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=3 * 32 * 32,\n",
    "        hidden_dim=100,\n",
    "        num_classes=10,\n",
    "        weight_scale=1e-3,\n",
    "        reg=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: An integer giving the size of the input\n",
    "        - hidden_dim: An integer giving the size of the hidden layer\n",
    "        - num_classes: An integer giving the number of classes to classify\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "          initialization of the weights.\n",
    "        - reg: Scalar giving L2 regularization strength.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "        \n",
    "        # Initialize weights using Gaussian with mean 0.0 and standard deviation weight_scale\n",
    "        # Initialize bias to zero\n",
    "        # Store weights and biases in the dictionary self.params using names 'W1' and 'b1' for first layer and \n",
    "        #'W2' and 'b2' for second layer\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient for a minibatch of data.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Array of input data of shape (N, d_1, ..., d_k)\n",
    "        - y: Array of labels of shape (N,)\n",
    "\n",
    "        Returns:\n",
    "        when y is None,it is test-time prediction, just return scores\n",
    "        A tuple of:\n",
    "        - loss: Scalar value giving the loss\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "          names to gradients of the loss with respect to those parameters.\n",
    "        \"\"\"\n",
    "        scores = None\n",
    "        # Forward pass\n",
    "        # Put the class scores in scores variable\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # juet return scores if y is None\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0, {}\n",
    "        \n",
    "        # Compute loss, do not forget L2 regularization, use 0.5*self.reg for the loss\n",
    "        # Store the gradients in grads using keys 'W1' and 'b1' for first layer and 'W2' and 'b2' for second layer\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad26291b28e5d1ac742e697978f92aed",
     "grade": true,
     "grade_id": "cell-a0c0c6ec76f9d583",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(47)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "std = 1e-3\n",
    "model = NeuralNetwork(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'Error First layer weights'\n",
    "assert np.all(b1 == 0), 'Error First layer biases '\n",
    "assert W2_std < std / 10, 'Error Second layer weights '\n",
    "assert np.all(b2 == 0), 'Error Second layer biases'\n",
    "\n",
    "print('Test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Error test-time forward pass'\n",
    "\n",
    "print('Training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Error training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Error regularization loss'\n",
    "\n",
    "for reg in [0.0, 0.7]:\n",
    "    print('Gradient check with reg = ', reg)\n",
    "    model.reg = reg\n",
    "    loss, grads = model.loss(X, y)\n",
    "\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "        error = rel_error(grad_num, grads[name])\n",
    "        print('%s error: %.2e' % (name, error))\n",
    "        assert error < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe0d0727cb6612b8b89fc4beedc1c18f",
     "grade": false,
     "grade_id": "cell-2f530adedd42782e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = get_CIFAR10_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d84d080e85471ca60181756eb3379b13",
     "grade": false,
     "grade_id": "cell-66b80e988a0ccd65",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "\n",
    "# Tune the learning rate, num_epochs, regularization to get the best accuracy \n",
    "learning_rate = None\n",
    "num_epochs = None\n",
    "reg = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes, reg=reg)\n",
    "solver = Solver(model, data, optim_config={'learning_rate': learning_rate}, num_epochs=num_epochs)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c842c1cded8c6177df473418b445ab0",
     "grade": true,
     "grade_id": "cell-6899f989278e382b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_val_pred = np.argmax(model.loss(data['X_val']), axis=1)\n",
    "val_acc = (y_val_pred == data['y_val']).mean()\n",
    "print('Validation set accuracy: ', val_acc)\n",
    "assert val_acc >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27cc59a4055392c08877c905e7095140",
     "grade": true,
     "grade_id": "cell-31a83c76a6d858df",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(model.loss(data['X_test']), axis=1)\n",
    "test_acc = (y_test_pred == data['y_test']).mean()\n",
    "print('Test set accuracy: ', test_acc)\n",
    "assert test_acc >= 0.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
