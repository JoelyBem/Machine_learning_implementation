{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3d3c2e2a84ff92fa02220ae689dc414",
     "grade": false,
     "grade_id": "cell-38b4529793385ad1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
    "\n",
    "\n",
    "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5, error=1e-9):\n",
    "    \"\"\"\n",
    "    sample a few random elements and only return numerical\n",
    "    in this dimensions\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape])\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h  # increment by h\n",
    "        fxph = f(x)  # evaluate f(x + h)\n",
    "        x[ix] = oldval - h  # increment by h\n",
    "        fxmh = f(x)  # evaluate f(x - h)\n",
    "        x[ix] = oldval  # reset\n",
    "\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (\n",
    "            abs(grad_numerical) + abs(grad_analytic)\n",
    "        )\n",
    "        print(\n",
    "            \"numerical: %f analytic: %f, relative error: %e\"\n",
    "            % (grad_numerical, grad_analytic, rel_error)\n",
    "        )\n",
    "        assert rel_error < error\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulas:\n",
    "- Loss function: $$ RSS(w) = \\dfrac{1}{n} \\sum_{i=0}^{N}(y_i - \\bar{y_i})^2 $$\n",
    "- Partial derivative in w:\n",
    "$$ D_w = \\dfrac{-2}{n} \\sum_{i=0}^{N}x_i(y_i -  \\bar{y_i}) $$\n",
    "- Partial derivative in b:\n",
    "$$ D_b = \\dfrac{-2}{n} \\sum_{i=0}^{N}(y_i -  \\bar{y_i}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1159b5dc8c59570993ac39ae349c5037",
     "grade": false,
     "grade_id": "cell-3fb8ebd5eb97e246",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "X_train1, y_train1 = data.data, data.target\n",
    "w1 = np.random.randn(X_train1.shape[1]) * 0.0001\n",
    "b1 = np.random.randn(1) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "baf9b75ec047c1cdb887ecb2bc2043a1",
     "grade": false,
     "grade_id": "cell-5ffdf99ad7cdfd69",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mse_loss_naive(w, b, X, y, alpha=0):\n",
    "    \"\"\"\n",
    "    MSE loss function WITH FOR LOOPs\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    - gradient with respect to bias b\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0.0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    sum_dw = 0\n",
    "    sum_db = 0\n",
    "    sum_loss = 0\n",
    "    n = len(X)\n",
    "    for i in range(len(X)):\n",
    "        y_i_pred = w @ X[i] + b\n",
    "        sum_dw = sum_dw + X[i]*(y[i] - y_i_pred)\n",
    "        sum_db = sum_db + (y[i] - y_i_pred)\n",
    "        sum_loss = sum_loss + (y[i]    - (w @ X[i] + b))**2\n",
    "    dw = -2 * sum_dw / n + 2*alpha*w\n",
    "    db = -2 * sum_db / n \n",
    "    loss = sum_loss / n +  alpha*(w.T@w)\n",
    "    \n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Linear regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67581686e36845f8ef8aacd81c1a201b",
     "grade": true,
     "grade_id": "cell-079e42b153b67a60",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  1.876890730921511e-16\n",
      "Gradient check w\n",
      "numerical: -2.801914 analytic: -2.801914, relative error: 3.056307e-08\n",
      "numerical: -4.296088 analytic: -4.296087, relative error: 9.928392e-08\n",
      "numerical: 2.892062 analytic: 2.892060, relative error: 2.621463e-07\n",
      "numerical: -3.234125 analytic: -3.234125, relative error: 6.958758e-08\n",
      "numerical: -4.145423 analytic: -4.145424, relative error: 1.018838e-07\n",
      "numerical: -1.376396 analytic: -1.376394, relative error: 7.331243e-07\n",
      "numerical: -4.145423 analytic: -4.145424, relative error: 1.018838e-07\n",
      "numerical: -3.234125 analytic: -3.234125, relative error: 6.958758e-08\n",
      "numerical: -2.801914 analytic: -2.801914, relative error: 3.056307e-08\n",
      "numerical: -1.553186 analytic: -1.553188, relative error: 4.707326e-07\n",
      "numerical: -1.553186 analytic: -1.553188, relative error: 4.707326e-07\n",
      "numerical: -1.376396 analytic: -1.376394, relative error: 7.331243e-07\n",
      "numerical: -1.275044 analytic: -1.275044, relative error: 1.153266e-07\n",
      "numerical: -1.376396 analytic: -1.376394, relative error: 7.331243e-07\n",
      "numerical: -1.275044 analytic: -1.275044, relative error: 1.153266e-07\n",
      "Gradient check bias\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_naive(w1, b1, X_train1, y_train1, alpha=0)\n",
    "\n",
    "sk_loss = mean_squared_error(X_train1 @ w1 + b1, y_train1)\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Ridge regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ca5790893e5950341f7370bb3be167c",
     "grade": true,
     "grade_id": "cell-14a64cb1cfe70b98",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: 2.892370 analytic: 2.892369, relative error: 2.546077e-07\n",
      "numerical: -3.234194 analytic: -3.234194, relative error: 6.220639e-08\n",
      "numerical: -3.234194 analytic: -3.234194, relative error: 6.220639e-08\n",
      "numerical: -1.275160 analytic: -1.275160, relative error: 1.011536e-07\n",
      "numerical: -1.275160 analytic: -1.275160, relative error: 1.011536e-07\n",
      "numerical: -1.376473 analytic: -1.376471, relative error: 7.260322e-07\n",
      "numerical: -3.153157 analytic: -3.153157, relative error: 3.002676e-08\n",
      "numerical: -3.153157 analytic: -3.153157, relative error: 3.002676e-08\n",
      "numerical: -1.376473 analytic: -1.376471, relative error: 7.260322e-07\n",
      "numerical: -1.553389 analytic: -1.553391, relative error: 4.835459e-07\n",
      "numerical: -3.153157 analytic: -3.153157, relative error: 3.002676e-08\n",
      "numerical: -3.153157 analytic: -3.153157, relative error: 3.002676e-08\n",
      "numerical: -1.376473 analytic: -1.376471, relative error: 7.260322e-07\n",
      "numerical: -3.234194 analytic: -3.234194, relative error: 6.220639e-08\n",
      "numerical: -1.275160 analytic: -1.275160, relative error: 1.011536e-07\n",
      "Gradient check bias\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 1.027395e-09\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_naive(w1, b1, X_train1, y_train1, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_naive(w1, b1, X_train1, y_train1, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulas:\n",
    "- Loss function: $$ RSS(w) = \\dfrac{1}{n} \\sum_{i=0}^{N}(y_i - \\bar{y_i})^2 $$\n",
    "- Partial derivative in w:\n",
    "$$ D_w = \\dfrac{-2}{n} \\sum_{i=0}^{N}x_i(y_i -  \\bar{y_i}) $$\n",
    "- Partial derivative in b:\n",
    "$$ D_b = \\dfrac{-2}{n} \\sum_{i=0}^{N}(y_i -  \\bar{y_i}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21b3a09270262dbad9cb1c15c0fc69f8",
     "grade": false,
     "grade_id": "cell-1528a28f467d90c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mse_loss_vectorized(w, b, X, y, alpha=0):\n",
    "    \"\"\"\n",
    "    MSE loss function WITHOUT FOR LOOPs\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    - gradient with respect to bias b\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0.0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    n = len(X)\n",
    "    dw = -2 * (y - (X@w + b)) @ X/n + 2*alpha*w\n",
    "    db = -2 * np.sum((y -(X@w + b)), axis=0)/n\n",
    "    loss = (y - (X@w + b)).T @ (y - (X@w + b))/n + alpha*(w.T@w)\n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorised Linear regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  6.256302436405037e-17\n",
      "Gradient check w\n",
      "numerical: -4.145425 analytic: -4.145424, relative error: 5.169430e-08\n",
      "numerical: -3.153317 analytic: -3.153317, relative error: 1.438681e-08\n",
      "numerical: 2.892061 analytic: 2.892060, relative error: 1.678024e-07\n",
      "numerical: -1.275043 analytic: -1.275044, relative error: 2.413257e-07\n",
      "numerical: -0.315454 analytic: -0.315454, relative error: 1.136560e-08\n",
      "numerical: -2.801914 analytic: -2.801914, relative error: 3.056307e-08\n",
      "numerical: -2.801914 analytic: -2.801914, relative error: 3.056307e-08\n",
      "numerical: -1.275043 analytic: -1.275044, relative error: 2.413257e-07\n",
      "numerical: -4.145425 analytic: -4.145424, relative error: 5.169430e-08\n",
      "numerical: -1.275043 analytic: -1.275044, relative error: 2.413257e-07\n",
      "numerical: -1.553188 analytic: -1.553188, relative error: 2.279255e-09\n",
      "numerical: -4.145425 analytic: -4.145424, relative error: 5.169430e-08\n",
      "numerical: -3.234125 analytic: -3.234125, relative error: 1.334395e-08\n",
      "numerical: -4.296088 analytic: -4.296087, relative error: 1.460272e-08\n",
      "numerical: -3.153317 analytic: -3.153317, relative error: 1.438681e-08\n",
      "Gradient check bias\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)\n",
    "\n",
    "sk_loss = mean_squared_error(X_train1 @ w1 + b1, y_train1)\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ffc6cc1a998e8ed785bea62c0208ce04",
     "grade": true,
     "grade_id": "cell-41637ca21c8f938d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  6.256302436405037e-17\n",
      "Gradient check w\n",
      "numerical: -0.315454 analytic: -0.315454, relative error: 1.136560e-08\n",
      "numerical: -2.801914 analytic: -2.801914, relative error: 3.056307e-08\n",
      "numerical: -3.234125 analytic: -3.234125, relative error: 1.334395e-08\n",
      "numerical: -3.234125 analytic: -3.234125, relative error: 1.334395e-08\n",
      "numerical: -4.296088 analytic: -4.296087, relative error: 1.460272e-08\n",
      "numerical: -1.275043 analytic: -1.275044, relative error: 2.413257e-07\n",
      "numerical: -2.801914 analytic: -2.801914, relative error: 3.056307e-08\n",
      "numerical: -4.296088 analytic: -4.296087, relative error: 1.460272e-08\n",
      "numerical: -1.376395 analytic: -1.376394, relative error: 1.384222e-07\n",
      "numerical: -2.801914 analytic: -2.801914, relative error: 3.056307e-08\n",
      "numerical: -3.153317 analytic: -3.153317, relative error: 1.438681e-08\n",
      "numerical: -2.801914 analytic: -2.801914, relative error: 3.056307e-08\n",
      "numerical: -4.145425 analytic: -4.145424, relative error: 5.169430e-08\n",
      "numerical: -1.553188 analytic: -1.553188, relative error: 2.279255e-09\n",
      "numerical: -2.801914 analytic: -2.801914, relative error: 3.056307e-08\n",
      "Gradient check bias\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)\n",
    "\n",
    "sk_loss = mean_squared_error(X_train1 @ w1 + b1, y_train1)\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized ridge regression loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89b351bee282f05c3f74c9541b3a825e",
     "grade": true,
     "grade_id": "cell-bb0ff99c19a85e0f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: -0.315485 analytic: -0.315485, relative error: 1.334629e-07\n",
      "numerical: -4.296081 analytic: -4.296081, relative error: 9.291373e-09\n",
      "numerical: -4.145219 analytic: -4.145218, relative error: 5.395556e-08\n",
      "numerical: 2.892370 analytic: 2.892369, relative error: 1.602739e-07\n",
      "numerical: -3.234194 analytic: -3.234194, relative error: 5.963953e-09\n",
      "numerical: -4.296081 analytic: -4.296081, relative error: 9.291373e-09\n",
      "numerical: -3.153157 analytic: -3.153157, relative error: 1.182815e-09\n",
      "numerical: -2.802019 analytic: -2.802019, relative error: 3.359216e-08\n",
      "numerical: 2.892370 analytic: 2.892369, relative error: 1.602739e-07\n",
      "numerical: -4.145219 analytic: -4.145218, relative error: 5.395556e-08\n",
      "numerical: 2.892370 analytic: 2.892369, relative error: 1.602739e-07\n",
      "numerical: -4.145219 analytic: -4.145218, relative error: 5.395556e-08\n",
      "numerical: -1.275159 analytic: -1.275160, relative error: 2.554663e-07\n",
      "numerical: -1.553391 analytic: -1.553391, relative error: 1.515378e-08\n",
      "numerical: -4.145219 analytic: -4.145218, relative error: 5.395556e-08\n",
      "Gradient check bias\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n",
      "numerical: -304.267164 analytic: -304.267164, relative error: 4.295689e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dw1, db1 = mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w1, dw1, 15,  error=1e-5)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b1: mse_loss_vectorized(w1, b1, X_train1, y_train1, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b1, db1, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulas for logistic regression:\n",
    "- Loss for logistic regression $$Loss(w) = -\\frac{1}{N}[y\\log(\\sigma(wx + b)) + (1 - y)\\log(1 - \\sigma(wx + b))]$$\n",
    "- Partial derivative in w$$ \\dfrac {dLoss(w)}{dw_j} = \\frac{1}{N}[\\sigma(wx + b) - y]x_j $$\n",
    "- Partial derivative in b$$ \\dfrac {dLoss(w)}{db} = \\frac{1}{N}[\\sigma(wx + b) - y] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd86b658f3ec113c00c2ee2e195f550f",
     "grade": false,
     "grade_id": "cell-434ec399cf8aeea7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train2, y_train2 = data.data, data.target\n",
    "w2 = np.random.randn(X_train2.shape[1]) * 0.0001\n",
    "b2 = np.random.randn(1) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4357b941a04e6dc51a2683b7f4d91ed",
     "grade": false,
     "grade_id": "cell-6ec156568b0c6e29",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def log_loss_naive(w, b, X, y, alpha=0):\n",
    "    \"\"\"\n",
    "    log loss function WITH FOR LOOPs\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0\n",
    "    n = len(X)\n",
    "    for i in range(len(X)):\n",
    "        loss = loss - (y[i] * math.log(sigmoid(X[i] @ w + b)) + (1-y[i]) * math.log(1-sigmoid(X[i] @ w + b)) )/n + alpha * (w.T @ w)\n",
    "        dw = dw + (sigmoid(X[i] @ w + b) - y[i] )*X[i]/n + 2 * alpha * w\n",
    "        db = db + (sigmoid(X[i] @ w + b) - y[i] )/n\n",
    "        \n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97d1606d0c00119f8956e4ef60ad346f",
     "grade": true,
     "grade_id": "cell-d3c078eb2449ee61",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  3.181128844370586e-16\n",
      "Gradient check w\n",
      "numerical: 0.027126 analytic: 0.027126, relative error: 8.518583e-10\n",
      "numerical: -0.000969 analytic: -0.000969, relative error: 6.822252e-09\n",
      "numerical: -0.007812 analytic: -0.007812, relative error: 2.533635e-09\n",
      "numerical: 0.011255 analytic: 0.011255, relative error: 1.485902e-09\n",
      "numerical: 0.027126 analytic: 0.027126, relative error: 8.518583e-10\n",
      "numerical: -0.396888 analytic: -0.396888, relative error: 1.591155e-07\n",
      "numerical: 0.011255 analytic: 0.011255, relative error: 1.485902e-09\n",
      "numerical: -0.017990 analytic: -0.017990, relative error: 6.032011e-10\n",
      "numerical: -0.000231 analytic: -0.000231, relative error: 5.077697e-08\n",
      "numerical: 0.033353 analytic: 0.033353, relative error: 5.648010e-10\n",
      "numerical: -0.176035 analytic: -0.176035, relative error: 1.097467e-09\n",
      "numerical: -0.007812 analytic: -0.007812, relative error: 2.533635e-09\n",
      "numerical: -0.000969 analytic: -0.000969, relative error: 6.822252e-09\n",
      "numerical: 0.008658 analytic: 0.008658, relative error: 1.739218e-09\n",
      "numerical: 0.033353 analytic: 0.033353, relative error: 5.648010e-10\n",
      "Gradient check bias\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.118154e-10\n"
     ]
    }
   ],
   "source": [
    "y_pred_0 = sigmoid(X_train2 @ w2 + b2)\n",
    "y_pred = np.vstack([1-y_pred_0, y_pred_0]).T\n",
    "sk_loss = log_loss(y_train2, y_pred)\n",
    "\n",
    "loss, dw2, db2 = log_loss_naive(w2, b2, X_train2, y_train2, alpha=0)\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive with regulariztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb3c9584e2ca3502e5068d5c1ee2df96",
     "grade": true,
     "grade_id": "cell-b91805308ad91ec5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: 0.284840 analytic: 0.284840, relative error: 8.546001e-10\n",
      "numerical: -0.567309 analytic: -0.567309, relative error: 3.600676e-10\n",
      "numerical: 0.085603 analytic: 0.085603, relative error: 9.074147e-10\n",
      "numerical: -0.063277 analytic: -0.063277, relative error: 2.276913e-09\n",
      "numerical: 0.025017 analytic: 0.025017, relative error: 2.830086e-09\n",
      "numerical: -0.063277 analytic: -0.063277, relative error: 2.276913e-09\n",
      "numerical: 41.080936 analytic: 41.081001, relative error: 7.984152e-07\n",
      "numerical: -0.026063 analytic: -0.026063, relative error: 3.744631e-09\n",
      "numerical: -0.127078 analytic: -0.127078, relative error: 2.807716e-09\n",
      "numerical: 0.049020 analytic: 0.049020, relative error: 1.752950e-09\n",
      "numerical: 0.169846 analytic: 0.169846, relative error: 9.455659e-10\n",
      "numerical: 0.026036 analytic: 0.026036, relative error: 3.751112e-11\n",
      "numerical: 0.111200 analytic: 0.111200, relative error: 4.796508e-10\n",
      "numerical: 0.025017 analytic: 0.025017, relative error: 2.830086e-09\n",
      "numerical: -0.063277 analytic: -0.063277, relative error: 2.276913e-09\n",
      "Gradient check bias\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 1.342661e-10\n"
     ]
    }
   ],
   "source": [
    "loss, dw2, db2 = log_loss_naive(w2, b2, X_train2, y_train2, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b2: log_loss_naive(w2, b2, X_train2, y_train2, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulas for logistic regression:\n",
    "- Loss for logistic regression $$Loss(w) = -\\frac{1}{N}[y\\log(\\sigma(wx + b)) + (1 - y)\\log(1 - \\sigma(wx + b))]$$\n",
    "- Partial derivative in w$$ \\dfrac {dLoss(w)}{dw_j} = \\frac{1}{N}[\\sigma(wx + b) - y]x_j $$\n",
    "- Partial derivative in b$$ \\dfrac {dLoss(w)}{db} = \\frac{1}{N}[\\sigma(wx + b) - y] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f452aaa24d69e2257c0851d750fd4aa",
     "grade": false,
     "grade_id": "cell-a96e9a6d51919ffc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_loss_vectorized(w, b,X, y, alpha=0):\n",
    "    \"\"\"\n",
    "    log loss function WITHOUT FOR LOOPs\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss \n",
    "    - gradient with respect to weights w\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0\n",
    "    n = len(X)\n",
    "    # YOUR CODE HERE\n",
    "    loss = -(y @ np.log(sigmoid(X@w + b)) + (1 - y)@np.log(1 - sigmoid(X@w + b)))/n + alpha * (w.T @ w)\n",
    "    dw = ((sigmoid(X@w + b) - y) @ X)/n + 2 * alpha * w\n",
    "    db = np.sum((sigmoid(X@w + b) - y))/n\n",
    "    return loss, dw, np.array(db).reshape(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37c0df2fc2b67514a7bd11e120562fa1",
     "grade": true,
     "grade_id": "cell-ca14e49ada130789",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss error :  7.952822110926462e-17\n",
      "Gradient check w\n",
      "numerical: 0.027126 analytic: 0.027126, relative error: 1.356172e-10\n",
      "numerical: -0.152612 analytic: -0.152612, relative error: 2.319178e-11\n",
      "numerical: 0.008658 analytic: 0.008658, relative error: 1.362717e-10\n",
      "numerical: -0.396888 analytic: -0.396888, relative error: 1.591854e-07\n",
      "numerical: -0.002567 analytic: -0.002567, relative error: 1.092139e-10\n",
      "numerical: -0.152612 analytic: -0.152612, relative error: 2.319178e-11\n",
      "numerical: -0.007542 analytic: -0.007542, relative error: 4.277628e-10\n",
      "numerical: -0.396888 analytic: -0.396888, relative error: 1.591854e-07\n",
      "numerical: -0.000211 analytic: -0.000211, relative error: 5.570658e-09\n",
      "numerical: 0.013807 analytic: 0.013807, relative error: 3.394843e-11\n",
      "numerical: -0.017990 analytic: -0.017990, relative error: 1.393700e-11\n",
      "numerical: -1.806660 analytic: -1.806660, relative error: 2.346240e-10\n",
      "numerical: -0.007812 analytic: -0.007812, relative error: 3.086581e-10\n",
      "numerical: -1.514292 analytic: -1.514292, relative error: 1.122469e-10\n",
      "numerical: 0.027126 analytic: 0.027126, relative error: 1.356172e-10\n",
      "Gradient check bias\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n"
     ]
    }
   ],
   "source": [
    "y_pred_0 = sigmoid(X_train2 @ w2 + b2)\n",
    "y_pred = np.vstack([1-y_pred_0, y_pred_0]).T\n",
    "sk_loss = log_loss(y_train2, y_pred)\n",
    "\n",
    "loss, dw2, db2 = log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=0)\n",
    "assert rel_error(loss, sk_loss) < 1e-9\n",
    "print(\"Loss error : \",rel_error(loss, sk_loss))\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=0)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53e97adc2666d529eff9ba1abcd3756a",
     "grade": true,
     "grade_id": "cell-bce082fccf8b7057",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check w\n",
      "numerical: 0.013828 analytic: 0.013828, relative error: 1.930527e-11\n",
      "numerical: -1.514153 analytic: -1.514153, relative error: 1.117638e-10\n",
      "numerical: -0.011517 analytic: -0.011517, relative error: 2.638843e-10\n",
      "numerical: 0.013828 analytic: 0.013828, relative error: 1.930527e-11\n",
      "numerical: 0.008600 analytic: 0.008600, relative error: 3.398372e-10\n",
      "numerical: -0.000031 analytic: -0.000031, relative error: 7.759122e-08\n",
      "numerical: 0.013828 analytic: 0.013828, relative error: 1.930527e-11\n",
      "numerical: -2.551054 analytic: -2.551054, relative error: 1.223369e-08\n",
      "numerical: 41.121829 analytic: 41.121895, relative error: 7.976202e-07\n",
      "numerical: -0.001148 analytic: -0.001148, relative error: 2.658985e-09\n",
      "numerical: -0.000239 analytic: -0.000239, relative error: 1.247142e-08\n",
      "numerical: 95.768081 analytic: 95.768340, relative error: 1.351411e-06\n",
      "numerical: 0.013828 analytic: 0.013828, relative error: 1.930527e-11\n",
      "numerical: -0.007485 analytic: -0.007485, relative error: 3.022149e-10\n",
      "numerical: -0.007503 analytic: -0.007503, relative error: 3.473951e-10\n",
      "Gradient check bias\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n",
      "numerical: -0.123629 analytic: -0.123629, relative error: 2.288894e-11\n"
     ]
    }
   ],
   "source": [
    "loss, dw2, db2 = log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=1)\n",
    "\n",
    "print(\"Gradient check w\")\n",
    "# Check with numerical gradient w\n",
    "f = lambda w2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f, w2, dw2, 15, error=1e-4)\n",
    "\n",
    "print(\"Gradient check bias\")\n",
    "# Check with numerical gradient b\n",
    "f2 = lambda b2: log_loss_vectorized(w2, b2, X_train2, y_train2, alpha=1)[0]\n",
    "grad_numerical = grad_check_sparse(f2, b2, db2, 15,  error=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent for Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a1783db1a1f1898028260238567bed7",
     "grade": false,
     "grade_id": "cell-485e52c0efd4f4a9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearModel():\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, alpha=0, num_iters=100, batch_size=200, verbose=False):\n",
    "        N, d = X.shape\n",
    "        \n",
    "        if self.w is None: # Initialization\n",
    "            self.w = 0.001 * np.random.randn(d)\n",
    "            self.b = 0.0\n",
    "\n",
    "        # Run stochastic gradient descent to optimize w\n",
    "        \n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "                                                               \n",
    "            # Sample batch_size elements in X_batch and y_batch\n",
    "            # X_batch shape is  (batch_size, d) and y_batch shape is (batch_size,) \n",
    "            # Hint: Use np.random.choice to generate indices\n",
    "            # YOUR CODE HERE\n",
    "            n = len(X)\n",
    "            index = np.random.choice(range(n), size=batch_size)\n",
    "            X_batch = X[index]\n",
    "            y_batch = y[index]\n",
    "\n",
    "            # evaluate loss and gradient\n",
    "            loss, dw, db = self.loss(X_batch, y_batch, alpha)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # perform parameter update                                                                \n",
    "            # Update the weights w and bias b using the gradient and the learning rate. \n",
    "            # YOUR CODE HERE\n",
    "            self.w = self.w - learning_rate * dw\n",
    "            self.b = self.b - learning_rate * db\n",
    "            \n",
    "            if verbose and it % 10000 == 0:\n",
    "                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "        \n",
    "        print(\"Last loss = \", loss )\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        pass\n",
    "\n",
    "class LinearRegressor(LinearModel):\n",
    "    \"\"\" Linear regression \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, alpha):\n",
    "        return mse_loss_vectorized(self.w, self.b, X_batch, y_batch, alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # YOUR CODE HERE\n",
    "        y_predict = X@self.w + self.b\n",
    "        return y_predict\n",
    "\n",
    "class LogisticRegressor(LinearModel):\n",
    "    \"\"\" Linear regression \"\"\"\n",
    "\n",
    "    def loss(self, X_batch, y_batch, alpha):\n",
    "        return log_loss_vectorized(self.w, self.b, X_batch, y_batch, alpha)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Return prediction labels vector of 0 or 1 \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        proba_predict = sigmoid(self.w@X.T + self.b)\n",
    "        proba_predict[proba_predict>0.5] = 1\n",
    "        proba_predict[proba_predict<=0.5] = 0\n",
    "        return proba_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30f1c271b16efe670214663c10743937",
     "grade": true,
     "grade_id": "cell-92f36a3b387a4277",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 75000: loss 24195.788156\n",
      "iteration 10000 / 75000: loss 3706.175791\n",
      "iteration 20000 / 75000: loss 3892.324140\n",
      "iteration 30000 / 75000: loss 3830.015595\n",
      "iteration 40000 / 75000: loss 3465.152712\n",
      "iteration 50000 / 75000: loss 2669.135149\n",
      "iteration 60000 / 75000: loss 2184.448164\n",
      "iteration 70000 / 75000: loss 2687.917981\n",
      "Last loss =  3405.3924988387635\n",
      "MSE scikit-learn: 2859.6903987680657\n",
      "MSE gradient descent model : 2884.3613792931287\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sk_model = LinearRegression(fit_intercept=True)\n",
    "sk_model.fit(X_train1, y_train1)\n",
    "sk_pred = sk_model.predict(X_train1)\n",
    "sk_mse = mean_squared_error(sk_pred, y_train1)\n",
    "\n",
    "model = LinearRegressor()\n",
    "model.train(X_train1, y_train1, num_iters=75000, batch_size=64, learning_rate=1e-2, verbose=True)\n",
    "pred = model.predict(X_train1)\n",
    "mse = mean_squared_error(pred, y_train1)\n",
    "\n",
    "print(\"MSE scikit-learn:\", sk_mse)\n",
    "print(\"MSE gradient descent model :\", mse)\n",
    "assert mse - sk_mse < 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistc regression with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6196155f76f4eded31fefce67a76d60",
     "grade": true,
     "grade_id": "cell-925a2ddb5c1ba7ff",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 75000: loss 0.694033\n",
      "iteration 10000 / 75000: loss 0.076443\n",
      "iteration 20000 / 75000: loss 0.106687\n",
      "iteration 30000 / 75000: loss 0.048231\n",
      "iteration 40000 / 75000: loss 0.077529\n",
      "iteration 50000 / 75000: loss 0.087962\n",
      "iteration 60000 / 75000: loss 0.165647\n",
      "iteration 70000 / 75000: loss 0.036883\n",
      "Last loss =  0.1092023058226832\n",
      "Log-loss scikit-learn: 0.4249086712816093\n",
      "Log-loss gradiet descent model : 0.4249086712816093\n",
      "Error : 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train2 = scaler.fit_transform(X_train2)\n",
    "\n",
    "sk_model = LogisticRegression(fit_intercept=True)\n",
    "sk_model.fit(X_train2, y_train2)\n",
    "sk_pred = sk_model.predict(X_train2)\n",
    "sk_log_loss = log_loss(sk_pred, y_train2)\n",
    "\n",
    "model = LogisticRegressor()\n",
    "model.train(X_train2, y_train2, num_iters=75000, batch_size=64, learning_rate=1e-3, verbose=True)\n",
    "pred = model.predict(X_train2)\n",
    "model_log_loss = log_loss(pred, y_train2)\n",
    "\n",
    "print(\"Log-loss scikit-learn:\", sk_log_loss)\n",
    "print(\"Log-loss gradiet descent model :\", model_log_loss)\n",
    "print(\"Error :\", rel_error(sk_log_loss, model_log_loss))\n",
    "assert rel_error(sk_log_loss, model_log_loss) < 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
